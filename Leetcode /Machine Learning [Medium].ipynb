{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e5ad15f-e2e1-4fed-8c2d-c3a09ef4917f",
   "metadata": {},
   "source": [
    "## Machine Learning - Medium "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79a5c86-b766-4dfc-832a-2244d0d9e866",
   "metadata": {},
   "source": [
    "K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb17cdd-1953-46f3-a950-5ab9d1e744f0",
   "metadata": {},
   "source": [
    "Write a Python function that implements the k-Means algorithm for clustering, starting with specified initial centroids and a set number of iterations. The function should take a list of points (each represented as a tuple of coordinates), an integer k representing the number of clusters to form, a list of initial centroids (each a tuple of coordinates), and an integer representing the maximum number of iterations to perform. The function will iteratively assign each point to the nearest centroid and update the centroids based on the assignments until the centroids do not change significantly, or the maximum number of iterations is reached. The function should return a list of the final centroids of the clusters. Round to the nearest fourth decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ce0662-070e-4421-8550-644c0d6fb261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculating Euclidean Distance \n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(((a - b) ** 2).sum(axis=1))\n",
    "\n",
    "# Main Method \n",
    "def k_means_clustering(points, k, initial_centroids, max_iterations):\n",
    "    # Create array of points\n",
    "    points = np.array(points)\n",
    "    # Create array of centroids\n",
    "    centroids = np.array(initial_centroids)\n",
    "    \n",
    "    # Update steps until the centroids no longer change significantly or max iterations reached\n",
    "    for iteration in range(max_iterations):\n",
    "        # Assign points to the nearest centroid\n",
    "        distances = np.array([euclidean_distance(points, centroid) for centroid in centroids])\n",
    "        assignments = np.argmin(distances, axis=0)\n",
    "\n",
    "        # Compute new centroids\n",
    "        new_centroids = np.array([points[assignments == i].mean(axis=0) if len(points[assignments == i]) > 0 else centroids[i] for i in range(k)])\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.all(centroids == new_centroids):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "        centroids = np.round(centroids, 4)\n",
    "    \n",
    "    return [tuple(centroid) for centroid in centroids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4d03c-41c5-4630-ac0f-6a3232a3e783",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75893563-13f9-4d6c-8df5-96f73f647ae8",
   "metadata": {},
   "source": [
    "Cross-Validation Data Split Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee7b60b-fb24-4bce-8038-857453321772",
   "metadata": {},
   "source": [
    "Write a Python function that performs k-fold cross-validation data splitting from scratch. The function should take a dataset (as a 2D NumPy array where each row represents a data sample and each column represents a feature) and an integer k representing the number of folds. The function should split the dataset into k parts, systematically use one part as the test set and the remaining as the training set, and return a list where each element is a tuple containing the training set and test set for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed3b4490-1230-4d6c-bcd5-81c2dd0c49d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_validation_split(data, k):\n",
    "    # np.random.shuffle(data)  # This line can be removed if shuffling is not desired in examples\n",
    "    fold_size = len(data) // k\n",
    "    folds = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        start, end = i * fold_size, (i + 1) * fold_size if i != k-1 else len(data)\n",
    "        test = data[start:end]\n",
    "        train = np.concatenate([data[:start], data[end:]])\n",
    "        folds.append([train.tolist(), test.tolist()])\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c5d9a-0572-44c4-88a6-424494cd064e",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8223ec-1b93-458a-8021-f20c25c2c847",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e7b99-6937-42d1-a901-c67763a162e7",
   "metadata": {},
   "source": [
    "Write a Python function that performs Principal Component Analysis (PCA) from scratch. The function should take a 2D NumPy array as input, where each row represents a data sample and each column represents a feature. The function should standardize the dataset, compute the covariance matrix, find the eigenvalues and eigenvectors, and return the principal components (the eigenvectors corresponding to the largest eigenvalues). The function should also take an integer k as input, representing the number of principal components to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f8ea97d-c13d-47eb-a0c1-02155b76d045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pca(data, k):\n",
    "    # Standardize the data\n",
    "    data_standardized = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n",
    "    \n",
    "    # Compute the covariance matrix\n",
    "    covariance_matrix = np.cov(data_standardized, rowvar=False)\n",
    "    \n",
    "    # Eigen decomposition\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "    \n",
    "    # Sort the eigenvectors by decreasing eigenvalues\n",
    "    idx = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues_sorted = eigenvalues[idx]\n",
    "    eigenvectors_sorted = eigenvectors[:,idx]\n",
    "    \n",
    "    # Select the top k eigenvectors (principal components)\n",
    "    principal_components = eigenvectors_sorted[:, :k]\n",
    "    \n",
    "    return np.round(principal_components, 4).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc1c53a-312a-487c-8a03-712334c314b7",
   "metadata": {},
   "source": [
    "eigenvalues: The eigenvalues of the covariance matrix, representing the amount of variance explained by each principal component.\n",
    "eigenvectors: The eigenvectors of the covariance matrix, representing the directions of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7339e0b-7879-43ba-a2d7-bdbbb0bb9855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
